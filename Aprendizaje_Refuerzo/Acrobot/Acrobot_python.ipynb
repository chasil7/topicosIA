{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El sistema acrobot consiste en una especie de brazo robotico que incluye dos uniones. Inicialmente, los enlaces cuelgan hacia abajo y el objetivo es mover el extremo del enlace inferior hasta una altura determinada.\n",
    "Gracias a las librerias de openAI Gym podemos preocuparnos solamente en el algoritmo de entrenamiento sin necesidad de programar en si al [Acrobot](https://gym.openai.com/envs/Acrobot-v1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Política Aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, una demostración del comportamiento del acrobot, el cual toma la decisión de moverse aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Acrobot-v1')\n",
    "for i_episode in range(20):\n",
    "    total_reward = 0.0\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            print (\"Episode {} finished after {} timesteps. Total reward: {}\".format(i_episode, t+1, total_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejor Política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxAction(Q,s):\n",
    "    values=np.array([Q[s,a] for a in [0,1,2]])\n",
    "    action = np.argmax(values)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzaremos discretizando el espacio de estados. Los estados son tuplas de 6 elementos, de los cuales los primeros cuatro son senos y cosenos de los ángulos que forman el brazo robótico. Éstos toman valores entre -1 y 1 y los otros dos, en su mayoría, según unas pruebas, entre -5 y 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[cos(theta1) sin(theta1) cos(theta2) sin(theta2) thetaDot1 thetaDot2].\n",
    "ct1_space=np.linspace(-1,1,10)\n",
    "st1_space=np.linspace(-1,1,10)\n",
    "ct2_space=np.linspace(-1,1,10)\n",
    "st2_space=np.linspace(-1,1,10)\n",
    "t1_space=np.linspace(-5,5,10)\n",
    "t2_space=np.linspace(-5,5,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función reciba la observación real del acrobot, es decir la tupla de 6 elementos, y regresa el estado con elementos enteros, que básicamente consisten en la posicion en donde se encuentra cada número flotante o real en los rangos definidos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getState(observation):\n",
    "    ct1=int(np.digitize(observation[0],ct1_space))\n",
    "    st1=int(np.digitize(observation[1],st1_space))\n",
    "    ct2=int(np.digitize(observation[2],ct2_space))\n",
    "    st2=int(np.digitize(observation[3],st2_space))\n",
    "    t1=int(np.digitize(observation[4],t1_space))\n",
    "    t2=int(np.digitize(observation[5],t2_space))\n",
    "    \n",
    "    return(ct1,st1,ct2,st2,t1,t2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el conjunto de estados e inicializamos nuestro diccionario Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states=[]\n",
    "\n",
    "for i in range(len(ct1_space)+1):\n",
    "    for j in range(len(st1_space)+1):\n",
    "        for k in range(len(ct2_space)+1):\n",
    "            for l in range(len(st2_space)+1):\n",
    "                for m in range(len(t1_space)+1):\n",
    "                    for n in range(len(t2_space)+1):\n",
    "                        states.append((i,j,k,l,m,n))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q={}\n",
    "for s in states:\n",
    "    for a in [0,1,2]:\n",
    "        Q[s,a]=0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sarsa(env,alpha, gamma, epsilon,episodes, Q):\n",
    "    \n",
    "    Rewards=np.zeros(episodes)\n",
    "    moves=np.zeros(episodes)\n",
    "    for i in range(episodes):\n",
    "        \n",
    "        observation=env.reset()\n",
    "        s=getState(observation)\n",
    "        rand=np.random.random()\n",
    "        a=maxAction(Q,s) if rand<1-epsilon else env.action_space.sample()\n",
    "        done=False\n",
    "        epReward=0\n",
    "        epMoves=0\n",
    "        while not done:\n",
    "            observation_,reward,done,info= env.step(a)\n",
    "            s_=getState(observation_)\n",
    "            rand=np.random.random()\n",
    "            a_=maxAction(Q,s) if rand<1-epsilon else env.action_space.sample()\n",
    "            epReward+=reward\n",
    "            epMoves+=1\n",
    "            \n",
    "            Q[s,a]=Q[s,a]+alpha*(reward + gamma*Q[s_,a_]-Q[s,a])\n",
    "            \n",
    "            s,a=s_,a_\n",
    "            \n",
    "        epsilon-=1/epsilon if epsilon>0 else 0\n",
    "        Rewards[i]=epReward\n",
    "        moves[i]=epMoves\n",
    "        \n",
    "        if i%100==0:\n",
    "            print(\"Episode {} finished after {} timesteps. Total reward: {}\".format(i, moves[i], Rewards[i]))\n",
    "    return Q,Rewards,moves\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para simular el acrobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def taste(env,Q, episodes):\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        done=False \n",
    "        observation=env.reset()\n",
    "        total_reward=0\n",
    "        moves=0\n",
    "        while not done:\n",
    "            action = maxAction(Q, getState(observation))\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            env.render()\n",
    "            total_reward += reward\n",
    "            moves+=1\n",
    "            if done:\n",
    "                print(\"Episode {} finished after {} timesteps. Total reward: {}\".format(i, moves, total_reward))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 100 finished after 373.0 timesteps. Total reward: -372.0\n",
      "Episode 200 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 300 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 400 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 500 finished after 449.0 timesteps. Total reward: -448.0\n",
      "Episode 600 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 700 finished after 483.0 timesteps. Total reward: -482.0\n",
      "Episode 800 finished after 265.0 timesteps. Total reward: -264.0\n",
      "Episode 900 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 1000 finished after 247.0 timesteps. Total reward: -246.0\n",
      "Episode 1100 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 1200 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 1300 finished after 283.0 timesteps. Total reward: -282.0\n",
      "Episode 1400 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 1500 finished after 401.0 timesteps. Total reward: -400.0\n",
      "Episode 1600 finished after 300.0 timesteps. Total reward: -299.0\n",
      "Episode 1700 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 1800 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 1900 finished after 252.0 timesteps. Total reward: -251.0\n",
      "Episode 2000 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 2100 finished after 325.0 timesteps. Total reward: -324.0\n",
      "Episode 2200 finished after 418.0 timesteps. Total reward: -417.0\n",
      "Episode 2300 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 2400 finished after 398.0 timesteps. Total reward: -397.0\n",
      "Episode 2500 finished after 244.0 timesteps. Total reward: -243.0\n",
      "Episode 2600 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 2700 finished after 375.0 timesteps. Total reward: -374.0\n",
      "Episode 2800 finished after 311.0 timesteps. Total reward: -310.0\n",
      "Episode 2900 finished after 348.0 timesteps. Total reward: -347.0\n",
      "Episode 3000 finished after 255.0 timesteps. Total reward: -254.0\n",
      "Episode 3100 finished after 206.0 timesteps. Total reward: -205.0\n",
      "Episode 3200 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 3300 finished after 495.0 timesteps. Total reward: -494.0\n",
      "Episode 3400 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 3500 finished after 411.0 timesteps. Total reward: -410.0\n",
      "Episode 3600 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 3700 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 3800 finished after 435.0 timesteps. Total reward: -434.0\n",
      "Episode 3900 finished after 497.0 timesteps. Total reward: -496.0\n",
      "Episode 4000 finished after 428.0 timesteps. Total reward: -427.0\n",
      "Episode 4100 finished after 309.0 timesteps. Total reward: -308.0\n",
      "Episode 4200 finished after 399.0 timesteps. Total reward: -398.0\n",
      "Episode 4300 finished after 477.0 timesteps. Total reward: -476.0\n",
      "Episode 4400 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 4500 finished after 453.0 timesteps. Total reward: -452.0\n",
      "Episode 4600 finished after 418.0 timesteps. Total reward: -417.0\n",
      "Episode 4700 finished after 457.0 timesteps. Total reward: -456.0\n",
      "Episode 4800 finished after 450.0 timesteps. Total reward: -449.0\n",
      "Episode 4900 finished after 319.0 timesteps. Total reward: -318.0\n",
      "Episode 5000 finished after 375.0 timesteps. Total reward: -374.0\n",
      "Episode 5100 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 5200 finished after 192.0 timesteps. Total reward: -191.0\n",
      "Episode 5300 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 5400 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 5500 finished after 321.0 timesteps. Total reward: -320.0\n",
      "Episode 5600 finished after 254.0 timesteps. Total reward: -253.0\n",
      "Episode 5700 finished after 267.0 timesteps. Total reward: -266.0\n",
      "Episode 5800 finished after 304.0 timesteps. Total reward: -303.0\n",
      "Episode 5900 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 6000 finished after 344.0 timesteps. Total reward: -343.0\n",
      "Episode 6100 finished after 431.0 timesteps. Total reward: -430.0\n",
      "Episode 6200 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 6300 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 6400 finished after 457.0 timesteps. Total reward: -456.0\n",
      "Episode 6500 finished after 420.0 timesteps. Total reward: -419.0\n",
      "Episode 6600 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 6700 finished after 311.0 timesteps. Total reward: -310.0\n",
      "Episode 6800 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 6900 finished after 421.0 timesteps. Total reward: -420.0\n",
      "Episode 7000 finished after 177.0 timesteps. Total reward: -176.0\n",
      "Episode 7100 finished after 478.0 timesteps. Total reward: -477.0\n",
      "Episode 7200 finished after 283.0 timesteps. Total reward: -282.0\n",
      "Episode 7300 finished after 321.0 timesteps. Total reward: -320.0\n",
      "Episode 7400 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 7500 finished after 440.0 timesteps. Total reward: -439.0\n",
      "Episode 7600 finished after 288.0 timesteps. Total reward: -287.0\n",
      "Episode 7700 finished after 221.0 timesteps. Total reward: -220.0\n",
      "Episode 7800 finished after 313.0 timesteps. Total reward: -312.0\n",
      "Episode 7900 finished after 237.0 timesteps. Total reward: -236.0\n",
      "Episode 8000 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 8100 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 8200 finished after 480.0 timesteps. Total reward: -479.0\n",
      "Episode 8300 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 8400 finished after 372.0 timesteps. Total reward: -371.0\n",
      "Episode 8500 finished after 401.0 timesteps. Total reward: -400.0\n",
      "Episode 8600 finished after 323.0 timesteps. Total reward: -322.0\n",
      "Episode 8700 finished after 190.0 timesteps. Total reward: -189.0\n",
      "Episode 8800 finished after 367.0 timesteps. Total reward: -366.0\n",
      "Episode 8900 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 9000 finished after 500.0 timesteps. Total reward: -500.0\n",
      "Episode 9100 finished after 332.0 timesteps. Total reward: -331.0\n",
      "Episode 9200 finished after 374.0 timesteps. Total reward: -373.0\n",
      "Episode 9300 finished after 322.0 timesteps. Total reward: -321.0\n",
      "Episode 9400 finished after 422.0 timesteps. Total reward: -421.0\n",
      "Episode 9500 finished after 446.0 timesteps. Total reward: -445.0\n",
      "Episode 9600 finished after 313.0 timesteps. Total reward: -312.0\n",
      "Episode 9700 finished after 379.0 timesteps. Total reward: -378.0\n",
      "Episode 9800 finished after 201.0 timesteps. Total reward: -200.0\n",
      "Episode 9900 finished after 500.0 timesteps. Total reward: -500.0\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('Acrobot-v1')\n",
    "episodes=10000 #12000+5000\n",
    "aplha=0.5\n",
    "gamma=0.8\n",
    "eps=0.1\n",
    "\n",
    "Q, Rewards, moves = sarsa(env,aplha, gamma, eps,episodes, Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 310 timesteps. Total reward: -309.0\n"
     ]
    }
   ],
   "source": [
    "taste(env,Q,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"Q_sarsa.npy\", Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Qsaved=np.load(\"Q_acrobot.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taste(env,Qsaved,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "0AE9802298424FBCA775A8DB41CEDFDF",
   "lastKernelId": "3b112e14-87d1-4b8e-b135-015f21afbb01"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
