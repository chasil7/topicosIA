{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstract type MDP end\n",
    "abstract type Estado end\n",
    "abstract type Accion end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $epsilon- Greedy$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ep_greedy (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ep_greedy(mdp::MDP,ϵ::Float64, s::Estado, q_value )\n",
    "        \n",
    "    if rand()<= ϵ\n",
    "        action = rand(aLegales(mdp,s))\n",
    "    else\n",
    "        # choose an action based on epsilon-greedy algorithm\n",
    "        values_=Dict( a => q_value[(s,a)] for a in aLegales(mdp,s) )\n",
    "        action = findmax(values_)[2]\n",
    "    end\n",
    "    action\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Softmax$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax()\n",
    "    0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politica (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function politica(mdp::MDP,q_value)\n",
    "    # encontrar la accion que maximice el q_value para cada estado\n",
    "    \n",
    "    s_legales=[s for s in mdp.states if !terminal(mdp,s)]\n",
    "    sx=rand(s_legales)\n",
    "\n",
    "    π_ = Dict(sx => rand(aLegales(mdp,sx)))\n",
    "      \n",
    "    for s in s_legales\n",
    "        values= Dict(a => q_value[(s,a)] for a in aLegales(mdp,s))\n",
    "        π_[s]=findmax(values)[2]\n",
    "    end\n",
    "        \n",
    "    π_\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sarsa (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play for an episode\n",
    "function sarsa(mdp::MDP,q_value, s0::Estado, ϵ::Float64, α::Float64, episodes,\n",
    "        random_start,runs)\n",
    "    \n",
    "    steps=zeros(episodes)\n",
    "    Rewards=zeros(episodes)\n",
    "    \n",
    "    for run ∈ 1:runs\n",
    "        ep = 1\n",
    "        while ep <= episodes\n",
    "            # track the total time steps in this episode\n",
    "            time = 0\n",
    "\n",
    "            # initialize state\n",
    "            if random_start\n",
    "                state=rand(mdp.states)\n",
    "                while terminal(mdp,state)\n",
    "                    state=rand(mdp.states)\n",
    "                end\n",
    "            else \n",
    "                state = s0\n",
    "            end\n",
    "\n",
    "            action=ep_greedy(mdp,ϵ,state,q_value) #se toma una accion posible epsilon-Greedy\n",
    "            rewards=0.0\n",
    "\n",
    "            # keep going until get to the goal state\n",
    "            while !terminal(mdp,state)\n",
    "\n",
    "                next_state = step(mdp,state, action)\n",
    "\n",
    "                if !terminal(mdp,next_state)\n",
    "                    rewards+=r(mdp,state, action,next_state)\n",
    "                    next_action=ep_greedy(mdp,ϵ,next_state, q_value)\n",
    "\n",
    "                    # Sarsa update\n",
    "                    q_value[(state,action)] += α * (r(mdp,state,action,next_state) + q_value[(next_state,next_action)] -\n",
    "                             q_value[(state,action)])\n",
    "\n",
    "                    action = next_action\n",
    "                else\n",
    "                    rew=r(mdp,next_state)\n",
    "                    q_value[(state,action)] += α * (r(mdp,state,action,next_state) + rew -\n",
    "                             q_value[(state,action)])\n",
    "                    rewards+=rew\n",
    "\n",
    "                end\n",
    "\n",
    "                state = next_state\n",
    "                time += 1\n",
    "\n",
    "            end\n",
    "            #println(\"episodio: $ep con $time\")\n",
    "            Rewards[ep]+=rewards\n",
    "            steps[ep]+=time\n",
    "            ep += 1\n",
    "        end\n",
    "    end\n",
    "    Rewards /= runs\n",
    "    steps /= runs\n",
    "\n",
    "    π_=politica(mdp,q_value)\n",
    "    return π_,steps,Rewards\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expected_sarsa (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function expected_sarsa(mdp::MDP,q_value, s0::Estado, ϵ::Float64, α::Float64,γ::Float64, \n",
    "        episodes,random_start,runs)\n",
    "    \n",
    "    \n",
    "    steps=zeros(episodes)\n",
    "    Rewards=zeros(episodes)\n",
    "\n",
    "    for run ∈ 1:runs\n",
    "        ep = 1\n",
    "        while ep <= episodes\n",
    "            # track the total time steps in this episode\n",
    "            time = 0\n",
    "\n",
    "            # initialize state\n",
    "            if random_start\n",
    "                state=rand(mdp.states)\n",
    "                while terminal(mdp,state)\n",
    "                    state=rand(mdp.states)\n",
    "                end\n",
    "            else \n",
    "                state = s0\n",
    "            end\n",
    "\n",
    "            action=ep_greedy(mdp,ϵ,state,q_value) #se toma una accion posible epsilon-Greedy\n",
    "\n",
    "            rewards = 0.0\n",
    "            # keep going until get to the goal state\n",
    "            while !terminal(mdp,state)\n",
    "                next_state = step(mdp,state, action)\n",
    "                reward=r(mdp,state,action,next_state)\n",
    "                rewards += reward\n",
    "\n",
    "                if !terminal(mdp,next_state)\n",
    "                    target = 0.0\n",
    "                    next_action=ep_greedy(mdp,ϵ,next_state, q_value)\n",
    "\n",
    "\n",
    "                    q_next = Dict(a=>q_value[(next_state,a)] for a in aLegales(mdp,next_state))\n",
    "                    max=findmax(q_next)[1]\n",
    "                    best_actions=[a for (a,b) in q_next if b==max]\n",
    "\n",
    "                    accionesL=aLegales(mdp,next_state)\n",
    "                    for a in accionesL\n",
    "                        if a in best_actions\n",
    "                            target += ((1.0 - ϵ ) / length(best_actions) + ϵ / length(accionesL)) * q_value[(next_state, a)]\n",
    "                        else\n",
    "                            target += ϵ / length(accionesL) * q_value[(next_state, a)]\n",
    "                        end\n",
    "                    end\n",
    "\n",
    "                    target *= γ\n",
    "                else\n",
    "                    #target=0\n",
    "                    target=r(mdp,next_state)\n",
    "                    next_action=nothing\n",
    "                end\n",
    "\n",
    "                q_value[(state, action)] += α *(reward + target - q_value[(state, action)])\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                time += 1\n",
    "\n",
    "            end\n",
    "            Rewards[ep]+=rewards\n",
    "            steps[ep]+=time\n",
    "            ep += 1\n",
    "\n",
    "        end\n",
    "    end\n",
    "    Rewards /= runs\n",
    "    steps /= runs\n",
    "                \n",
    "    π_=politica(mdp,q_value)\n",
    "    return π_,steps,Rewards\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q_learning (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function q_learning(mdp::MDP,q_value, s0::Estado, ϵ::Float64, α::Float64, γ::Float64, \n",
    "        episodes,random_start,runs)\n",
    "    \n",
    "    steps = zeros(episodes)\n",
    "    Rewards = zeros(episodes)\n",
    "    \n",
    "    for run ∈ 1:runs\n",
    "        ep = 1\n",
    "        while ep <= episodes\n",
    "            # track the total time steps in this episode\n",
    "            time = 0\n",
    "            rewards=0.0\n",
    "            # initialize state\n",
    "            if random_start\n",
    "                state=rand(mdp.states)\n",
    "                while terminal(mdp,state)\n",
    "                    state=rand(mdp.states)\n",
    "                end\n",
    "            else \n",
    "                state = s0\n",
    "            end\n",
    "\n",
    "            while !terminal(mdp,state)\n",
    "                action=ep_greedy(mdp,ϵ,state,q_value) #se toma una accion posible epsilon-Greedy\n",
    "                next_state = step(mdp,state, action)\n",
    "                reward=r(mdp,state,action,next_state)\n",
    "                rewards += reward\n",
    "                # Q-Learning update\n",
    "                q_value[(state,action)]+= α * (reward + γ * maximum([q_value[next_state,a] for a in aLegales(mdp,next_state)]) -\n",
    "                    q_value[state, action])\n",
    "                #q_value[(state,action)]+= α * (reward + γ * maximum(q_value[(next_state,:)])  -\n",
    "                #    q_value[(state, action)])\n",
    "                state = next_state\n",
    "                time += 1\n",
    "\n",
    "            end\n",
    "            Rewards[ep]+=rewards\n",
    "            steps[ep]+=time\n",
    "            ep += 1\n",
    "        end\n",
    "    end\n",
    "    Rewards /= runs\n",
    "    steps /= runs\n",
    "\n",
    "    \n",
    "    π_=politica(mdp,q_value)\n",
    "    return π_,steps, Rewards\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semi_gradient_n_step_sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "semi_gradient_n_step_sarsa (generic function with 2 methods)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function semi_gradient_n_step_sarsa(mdp::MDP,value_function,funAction,ϵ,n=1)\n",
    "    \n",
    "    s=getInit()\n",
    "\n",
    "    a= funAction(mc,s, value_function, ϵ)\n",
    "    \n",
    "    states=[s]\n",
    "    actions = [a]\n",
    "    rewards = [0.0]\n",
    "    \n",
    "    time = 1\n",
    "    \n",
    "    T = 9000\n",
    "    \n",
    "    while true\n",
    "        time += 1\n",
    "        if time <= T\n",
    "            next_s, reward = step(mc,s, a)\n",
    "            next_a = funAction(mc,next_s, value_function, ϵ)\n",
    "            \n",
    "            # track new state and action\n",
    "            \n",
    "            push!(states, next_s)\n",
    "            push!(actions, next_a)\n",
    "            push!(rewards, reward)\n",
    "\n",
    "            if terminal(mc,next_s)\n",
    "                T = time\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        update_time = time - n\n",
    "        if update_time >= 0\n",
    "            returns = 0.0\n",
    "            # calculate corresponding rewards\n",
    "            for t in (update_time):(min(T, update_time + n))\n",
    "                returns += rewards[t]\n",
    "            end\n",
    "            \n",
    "            # add estimated state action value to the return\n",
    "            if update_time + n <= T \n",
    "                returns += value(value_function,mc,states[update_time + n],actions[update_time + n])\n",
    "            end\n",
    "\n",
    "            # update the state value function\n",
    "            if !terminal(mc,states[update_time])\n",
    "                learn(value_function,states[update_time], actions[update_time],\n",
    "                      returns)\n",
    "            end\n",
    "        end\n",
    "        if update_time == T - 1\n",
    "            break\n",
    "        end\n",
    "        s = next_s\n",
    "        a = next_a\n",
    "        \n",
    "    end\n",
    "    return time\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "1342BE3EA1FD4442BD68612633842D03",
   "lastKernelId": "145683c2-2b2b-481c-bcf6-44bdd575cb30"
  },
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
